# Model parameters similar to those of Wu et al. (2021; "A Smaller
# Transformer"):
#
# https://aclanthology.org/2021.eacl-main.163/
#
# They use --data.batch_size 100.
#
# This uses a feature-invariant transformer shared by source and features.
class_path: yoyodyne.models.TransformerModel
init_args:
  source_encoder:
    class_path: yoyodyne.models.modules.FeatureInvariantTransformerEncoder
    init_args:
      dropout: ${init_args.decoder_dropout}
      embedding_size: ${init_args.embedding_size}
      hidden_size: ${init_args.decoder_hidden_size}
      layers: ${init_args.decoder_layers}
  features_encoder: true
  decoder_dropout: .3
  decoder_hidden_size: 100
  decoder_layers: 4
  embedding_size: 256
  label_smoothing: .1
  optimizer:
    class_path: torch.optim.Adam
    init_args:
      lr: .001
  scheduler:
    class_path: yoyodyne.schedulers.WarmupInverseSquareRoot
    init_args:
      warmup_epochs: 10
