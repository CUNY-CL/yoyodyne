# Model parameters similar to those of Kann & Sch√ºtze (MED):
# https://aclanthology.org/W16-2010/
# 
# They use --data.batch_size 20.
# 
# Nowadays most people would probably prefer larger batch sizes and would use
# the Adam optimizer.
#
# This uses a separate linear features encoder.
class_path: yoyodyne.models.AttentiveLSTMModel
init_args:
  source_encoder:
    class_path: yoyodyne.models.modules.LSTMEncoder
    init_args:
      dropout: .3
      embedding_size: 300
      hidden_size: 100
  features_encoder:
    class_path: yoyodyne.models.modules.LinearEncoder
    init_args:
      embedding_size: 300
      # Has to be 2x source encoder embedding size because of
      # bidirectionality.
      output_size: 200
  decoder_dropout: .3
  decoder_hidden_size: 100
  embedding_size: 300
  optimizer:
    class_path: torch.optim.Adadelta
    init_args:
      lr: 1
